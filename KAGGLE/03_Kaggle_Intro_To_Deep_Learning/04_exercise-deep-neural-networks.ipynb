{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Intro to Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/deep-neural-networks).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction #\n\nIn the tutorial, we saw how to build deep neural networks by stacking layers inside a `Sequential` model. By adding an *activation function* after the hidden layers, we gave the network the ability to learn more complex (non-linear) relationships in the data.\n\nIn these exercises, you'll build a neural network with several hidden layers and then explore some activation functions beyond ReLU. Run this next cell to set everything up!","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n# Setup plotting\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\n\n# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.deep_learning_intro.ex2 import *","metadata":{"execution":{"iopub.status.busy":"2021-10-25T11:44:41.479788Z","iopub.execute_input":"2021-10-25T11:44:41.480155Z","iopub.status.idle":"2021-10-25T11:44:46.974751Z","shell.execute_reply.started":"2021-10-25T11:44:41.480070Z","shell.execute_reply":"2021-10-25T11:44:46.974087Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"In the *Concrete* dataset, your task is to predict the compressive strength of concrete manufactured according to various recipes.\n\nRun the next code cell without changes to load the dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nconcrete = pd.read_csv('../input/dl-course-data/concrete.csv')\nconcrete.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T11:45:16.460265Z","iopub.execute_input":"2021-10-25T11:45:16.460547Z","iopub.status.idle":"2021-10-25T11:45:16.504369Z","shell.execute_reply.started":"2021-10-25T11:45:16.460519Z","shell.execute_reply":"2021-10-25T11:45:16.503695Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"concrete.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-25T11:45:19.668658Z","iopub.execute_input":"2021-10-25T11:45:19.669460Z","iopub.status.idle":"2021-10-25T11:45:19.675440Z","shell.execute_reply.started":"2021-10-25T11:45:19.669417Z","shell.execute_reply":"2021-10-25T11:45:19.674851Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 1) Input Shape #\n\nThe target for this task is the column `'CompressiveStrength'`. The remaining columns are the features we'll use as inputs.\n\nWhat would be the input shape for this dataset?","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE\ninput_shape = (8,)\n\n# Check your answer\nq_1.check()","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2021-10-25T11:45:23.461162Z","iopub.execute_input":"2021-10-25T11:45:23.461716Z","iopub.status.idle":"2021-10-25T11:45:23.470884Z","shell.execute_reply.started":"2021-10-25T11:45:23.461678Z","shell.execute_reply":"2021-10-25T11:45:23.470011Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T10:52:08.865434Z","iopub.execute_input":"2021-10-25T10:52:08.865846Z","iopub.status.idle":"2021-10-25T10:52:08.874305Z","shell.execute_reply.started":"2021-10-25T10:52:08.865799Z","shell.execute_reply":"2021-10-25T10:52:08.873387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Define a Model with Hidden Layers #\n\nNow create a model with three hidden layers, each having 512 units and the ReLU activation.  Be sure to include an output layer of one unit and no activation, and also `input_shape` as an argument to the first layer.","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\n# YOUR CODE HERE\nmodel = keras.Sequential([\n    layers.Dense(512, activation='relu', input_shape=[8]),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(units=1),\n])\n\n# Check your answer\nq_2.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2021-10-25T11:45:29.010786Z","iopub.execute_input":"2021-10-25T11:45:29.011056Z","iopub.status.idle":"2021-10-25T11:45:30.311688Z","shell.execute_reply.started":"2021-10-25T11:45:29.011028Z","shell.execute_reply":"2021-10-25T11:45:30.310836Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_2.hint()\n#q_2.solution()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Activation Layers #\n\nLet's explore activations functions some.\n\nThe usual way of attaching an activation function to a `Dense` layer is to include it as part of the definition with the `activation` argument. Sometimes though you'll want to put some other layer between the `Dense` layer and its activation function. (We'll see an example of this in Lesson 5 with *batch normalization*.) In this case, we can define the activation in its own `Activation` layer, like so:\n\n```\nlayers.Dense(units=8),\nlayers.Activation('relu')\n```\n\nThis is completely equivalent to the ordinary way: `layers.Dense(units=8, activation='relu')`.\n\nRewrite the following model so that each activation is in its own `Activation` layer.","metadata":{}},{"cell_type":"code","source":"### YOUR CODE HERE: rewrite this to use activation layers\nmodel = keras.Sequential([\n    layers.Dense(32, activation='relu', input_shape=[8]),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(1),\n])\n\nmodel = keras.Sequential([\n    layers.Dense(32, input_shape=[8]),\n    layers.Activation('relu'),\n    layers.Dense(32),\n    layers.Activation('relu'),\n    layers.Dense(1),\n])\n\n# Check your answer\nq_3.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2021-10-25T11:45:47.424442Z","iopub.execute_input":"2021-10-25T11:45:47.424719Z","iopub.status.idle":"2021-10-25T11:45:47.493568Z","shell.execute_reply.started":"2021-10-25T11:45:47.424692Z","shell.execute_reply":"2021-10-25T11:45:47.492626Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2021-10-25T10:58:41.588948Z","iopub.execute_input":"2021-10-25T10:58:41.589996Z","iopub.status.idle":"2021-10-25T10:58:41.599226Z","shell.execute_reply.started":"2021-10-25T10:58:41.589929Z","shell.execute_reply":"2021-10-25T10:58:41.598447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T11:06:31.139251Z","iopub.execute_input":"2021-10-25T11:06:31.139886Z","iopub.status.idle":"2021-10-25T11:06:31.144346Z","shell.execute_reply.started":"2021-10-25T11:06:31.13984Z","shell.execute_reply":"2021-10-25T11:06:31.1433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optional: Alternatives to ReLU #\n\nThere is a whole family of variants of the `'relu'` activation -- `'elu'`, `'selu'`, and `'swish'`, among others -- all of which you can use in Keras. Sometimes one activation will perform better than another on a given task, so you could consider experimenting with activations as you develop a model. The ReLU activation tends to do well on most problems, so it's a good one to start with.\n\nLet's look at the graphs of some of these. Change the activation from `'relu'` to one of the others named above. Then run the cell to see the graph. (Check out the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/activations) for more ideas.)","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE: Change 'relu' to 'elu', 'selu', 'swish'... or something else\nactivation_layer = layers.Activation('relu')\n\nx = tf.linspace(-3.0, 3.0, 100)\ny = activation_layer(x) # once created, a layer is callable just like a function\n\nplt.figure(dpi=100)\nplt.plot(x, y)\nplt.xlim(-3, 3)\nplt.xlabel(\"Input\")\nplt.ylabel(\"Output\")\nplt.show()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2021-10-25T11:45:53.795797Z","iopub.execute_input":"2021-10-25T11:45:53.796140Z","iopub.status.idle":"2021-10-25T11:45:54.180170Z","shell.execute_reply.started":"2021-10-25T11:45:53.796109Z","shell.execute_reply":"2021-10-25T11:45:54.179067Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# YOUR CODE HERE: Change 'relu' to 'elu', 'selu', 'swish'... or something else\nactivation_layer = layers.Activation('elu')\n\nx = tf.linspace(-3.0, 3.0, 100)\ny = activation_layer(x) # once created, a layer is callable just like a function\n\nplt.figure(dpi=100)\nplt.plot(x, y)\nplt.xlim(-3, 3)\nplt.xlabel(\"Input\")\nplt.ylabel(\"Output\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T11:45:58.785219Z","iopub.execute_input":"2021-10-25T11:45:58.786037Z","iopub.status.idle":"2021-10-25T11:45:59.073336Z","shell.execute_reply.started":"2021-10-25T11:45:58.785988Z","shell.execute_reply":"2021-10-25T11:45:59.072243Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# YOUR CODE HERE: Change 'relu' to 'elu', 'selu', 'swish'... or something else\nactivation_layer = layers.Activation('selu')\n\nx = tf.linspace(-3.0, 3.0, 100)\ny = activation_layer(x) # once created, a layer is callable just like a function\n\nplt.figure(dpi=100)\nplt.plot(x, y)\nplt.xlim(-3, 3)\nplt.xlabel(\"Input\")\nplt.ylabel(\"Output\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T11:46:03.587584Z","iopub.execute_input":"2021-10-25T11:46:03.587904Z","iopub.status.idle":"2021-10-25T11:46:03.834963Z","shell.execute_reply.started":"2021-10-25T11:46:03.587872Z","shell.execute_reply":"2021-10-25T11:46:03.834044Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# YOUR CODE HERE: Change 'relu' to 'elu', 'selu', 'swish'... or something else\nactivation_layer = layers.Activation('swish')\n\nx = tf.linspace(-3.0, 3.0, 100)\ny = activation_layer(x) # once created, a layer is callable just like a function\n\nplt.figure(dpi=100)\nplt.plot(x, y)\nplt.xlim(-3, 3)\nplt.xlabel(\"Input\")\nplt.ylabel(\"Output\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T11:46:07.233717Z","iopub.execute_input":"2021-10-25T11:46:07.234302Z","iopub.status.idle":"2021-10-25T11:46:07.497616Z","shell.execute_reply.started":"2021-10-25T11:46:07.234250Z","shell.execute_reply":"2021-10-25T11:46:07.496761Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Keep Going #\n\nNow move on to Lesson 3 and [**learn how to train neural networks**](https://www.kaggle.com/ryanholbrook/stochastic-gradient-descent) with stochastic gradient descent.","metadata":{}}]}
{"metadata":{"jupytext":{"cell_metadata_filter":"-all","formats":"ipynb"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Intro to Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/binary-classification).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction #\n\nIn this exercise, you'll build a model to predict hotel cancellations with a binary classifier.","metadata":{}},{"cell_type":"code","source":"# Setup plotting\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('animation', html='html5')\n\n# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.deep_learning_intro.ex6 import *","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2021-10-25T20:56:01.688678Z","iopub.execute_input":"2021-10-25T20:56:01.688977Z","iopub.status.idle":"2021-10-25T20:56:01.729522Z","shell.execute_reply.started":"2021-10-25T20:56:01.688901Z","shell.execute_reply":"2021-10-25T20:56:01.728681Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"First, load the *Hotel Cancellations* dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nhotel = pd.read_csv('../input/dl-course-data/hotel.csv')\n\nX = hotel.copy()\ny = X.pop('is_canceled')\n\nX['arrival_date_month'] = \\\n    X['arrival_date_month'].map(\n        {'January':1, 'February': 2, 'March':3,\n         'April':4, 'May':5, 'June':6, 'July':7,\n         'August':8, 'September':9, 'October':10,\n         'November':11, 'December':12}\n    )\n\nfeatures_num = [\n    \"lead_time\", \"arrival_date_week_number\",\n    \"arrival_date_day_of_month\", \"stays_in_weekend_nights\",\n    \"stays_in_week_nights\", \"adults\", \"children\", \"babies\",\n    \"is_repeated_guest\", \"previous_cancellations\",\n    \"previous_bookings_not_canceled\", \"required_car_parking_spaces\",\n    \"total_of_special_requests\", \"adr\",\n]\nfeatures_cat = [\n    \"hotel\", \"arrival_date_month\", \"meal\",\n    \"market_segment\", \"distribution_channel\",\n    \"reserved_room_type\", \"deposit_type\", \"customer_type\",\n]\n\ntransformer_num = make_pipeline(\n    SimpleImputer(strategy=\"constant\"), # there are a few missing values\n    StandardScaler(),\n)\ntransformer_cat = make_pipeline(\n    SimpleImputer(strategy=\"constant\", fill_value=\"NA\"),\n    OneHotEncoder(handle_unknown='ignore'),\n)\n\npreprocessor = make_column_transformer(\n    (transformer_num, features_num),\n    (transformer_cat, features_cat),\n)\n\n# stratify - make sure classes are evenlly represented across splits\nX_train, X_valid, y_train, y_valid = \\\n    train_test_split(X, y, stratify=y, train_size=0.75)\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\n\ninput_shape = [X_train.shape[1]]","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2021-10-25T21:01:14.615416Z","iopub.execute_input":"2021-10-25T21:01:14.615976Z","iopub.status.idle":"2021-10-25T21:01:15.604762Z","shell.execute_reply.started":"2021-10-25T21:01:14.615939Z","shell.execute_reply":"2021-10-25T21:01:15.604023Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"input_shape = [X_train.shape[1]]\ninput_shape","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:02:02.288014Z","iopub.execute_input":"2021-10-25T21:02:02.288432Z","iopub.status.idle":"2021-10-25T21:02:02.297010Z","shell.execute_reply.started":"2021-10-25T21:02:02.288394Z","shell.execute_reply":"2021-10-25T21:02:02.295733Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# 1) Define Model #\n\nThe model we'll use this time will have both batch normalization and dropout layers. To ease reading we've broken the diagram into blocks, but you can define it layer by layer as usual.\n\nDefine a model with an architecture given by this diagram:\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/V04o59Z.png\" width=\"400\" alt=\"Diagram of network architecture: BatchNorm, Dense, BatchNorm, Dropout, Dense, BatchNorm, Dropout, Dense.\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Diagram of a binary classifier.</center></figcaption>\n</figure>\n","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\n# YOUR CODE HERE: define the model given in the diagram\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(1, activation='sigmoid'),\n\n])\n\n# Check your answer\nq_1.check()","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2021-10-25T21:07:29.294822Z","iopub.execute_input":"2021-10-25T21:07:29.295088Z","iopub.status.idle":"2021-10-25T21:07:36.626330Z","shell.execute_reply.started":"2021-10-25T21:07:29.295058Z","shell.execute_reply":"2021-10-25T21:07:36.625490Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# 2) Add Optimizer, Loss, and Metric #\n\nNow compile the model with the Adam optimizer and binary versions of the cross-entropy loss and accuracy metric.","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\n\n# Check your answer\nq_2.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2021-10-25T21:09:07.113933Z","iopub.execute_input":"2021-10-25T21:09:07.114186Z","iopub.status.idle":"2021-10-25T21:09:07.133958Z","shell.execute_reply.started":"2021-10-25T21:09:07.114156Z","shell.execute_reply":"2021-10-25T21:09:07.133184Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_2.hint()\n#q_2.solution()","metadata":{"lines_to_next_cell":0},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, run this cell to train the model and view the learning curves. It may run for around 60 to 70 epochs, which could take a minute or two.","metadata":{}},{"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=200,\n    callbacks=[early_stopping],\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:09:24.783146Z","iopub.execute_input":"2021-10-25T21:09:24.783744Z","iopub.status.idle":"2021-10-25T21:10:00.609002Z","shell.execute_reply.started":"2021-10-25T21:09:24.783703Z","shell.execute_reply":"2021-10-25T21:10:00.608261Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# 3) Train and Evaluate #\n\n\nWhat do you think about the learning curves? Does it look like the model underfit or overfit? Was the cross-entropy loss a good stand-in for accuracy?","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this cell to receive credit!)\nq_3.check()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T21:11:04.890464Z","iopub.execute_input":"2021-10-25T21:11:04.890818Z","iopub.status.idle":"2021-10-25T21:11:04.898533Z","shell.execute_reply.started":"2021-10-25T21:11:04.890783Z","shell.execute_reply":"2021-10-25T21:11:04.897516Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion #\n\nCongratulations! You've completed Kaggle's *Introduction to Deep Learning* course!\n\nWith your new skills you're ready to take on more advanced applications like computer vision and sentiment classification. What would you like to do next?\n\nWhy not try one of our *Getting Started* competitions?\n\n- Classify images with TPUs in [**Petals to the Metal**](https://www.kaggle.com/c/tpu-getting-started)\n- Create art with GANs in [**I'm Something of a Painter Myself**](https://www.kaggle.com/c/gan-getting-started)\n- Classify Tweets in [**Real or Not? NLP with Disaster Tweets**](https://www.kaggle.com/c/nlp-getting-started)\n- Detect contradiction and entailment in [**Contradictory, My Dear Watson**](https://www.kaggle.com/c/contradictory-my-dear-watson)\n\nUntil next time, Kagglers!","metadata":{}}]}